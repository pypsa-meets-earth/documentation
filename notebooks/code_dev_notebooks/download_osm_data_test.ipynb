{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Use This Notebook\n",
    "\n",
    "1. Copy download_osm_data.py into the first cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script does the following\n",
    "# 1. Downloads OSM files for specified countries from Geofabrik\n",
    "# 2. Filters files for substations and lines\n",
    "# 3. Process and clean data\n",
    "# 4. Exports to CSV\n",
    "# 5. Exports to GeoJson\n",
    "\n",
    "\"\"\"\n",
    "OSM extraction scrpt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# IMPORTANT: RUN SCRIPT FROM THIS SCRIPTS DIRECTORY i.e data_exploration/ TODO: make more robust\n",
    "# os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(\"../\")  # to import helpers\n",
    "\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from esy.osmfilter import run_filter\n",
    "from esy.osmfilter import Node, Relation, Way\n",
    "from esy.osmfilter import osm_info as osm_info\n",
    "from esy.osmfilter import osm_pickle as osm_pickle\n",
    "from config_osm_data import world_countries\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# https://gitlab.com/dlr-ve-esy/esy-osmfilter/-/tree/master/\n",
    "\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig()\n",
    "# logger=logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.WARNING)\n",
    "\n",
    "# Downloads PBF File for given Country Code\n",
    "\n",
    "\n",
    "def download_pbf(country_code, update):\n",
    "    \"\"\"\n",
    "    Downloads the pbf file from geofabrik for a given country code (see scripts/config_osm_data.py).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    country_code : str\n",
    "    update : bool\n",
    "        name of the network component\n",
    "        update = true forces re-download of files\n",
    "    \"\"\"\n",
    "    country_name = world_countries[country_code]\n",
    "    # Filename for geofabrik\n",
    "    geofabrik_filename = f\"{country_name}-latest.osm.pbf\"\n",
    "    # https://download.geofabrik.de/africa/nigeria-latest.osm.pbf\n",
    "    geofabrik_url = f\"https://download.geofabrik.de/africa/{geofabrik_filename}\"\n",
    "    PBF_inputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", \"pbf\", geofabrik_filename\n",
    "    )  # Input filepath\n",
    "\n",
    "    if not os.path.exists(PBF_inputfile) or update is True:\n",
    "        print(f\"{geofabrik_filename} does not exist, downloading to {PBF_inputfile}\")\n",
    "        #  create data/osm directory\n",
    "        os.makedirs(os.path.dirname(PBF_inputfile), exist_ok=True)\n",
    "        with requests.get(geofabrik_url, stream=True) as r:\n",
    "            with open(PBF_inputfile, \"wb\") as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return PBF_inputfile\n",
    "\n",
    "\n",
    "def download_and_filter(country_code, update=False):\n",
    "    PBF_inputfile = download_pbf(country_code, update)\n",
    "\n",
    "    filter_file_exists = False\n",
    "    # json file for the Data dictionary\n",
    "    JSON_outputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", country_code + \"_power.json\"\n",
    "    )  # json file for the Elements dictionary is automatically written to \"data/osm/Elements\"+filename)\n",
    "\n",
    "    if os.path.exists(JSON_outputfile):\n",
    "        filter_file_exists = True\n",
    "\n",
    "    # Load Previously Pre-Filtered Files\n",
    "    if update is False and filter_file_exists is True:\n",
    "        create_elements = False  # Do not create elements again\n",
    "        new_prefilter_data = False  # Do not pre-filter data again\n",
    "        # HACKY: esy.osmfilter code to re-create Data.pickle\n",
    "        Data = osm_info.ReadJason(JSON_outputfile, verbose=\"no\")\n",
    "        DataDict = {\"Data\": Data}\n",
    "        osm_pickle.picklesave(\n",
    "            DataDict,\n",
    "            os.path.realpath(\n",
    "                os.path.join(os.getcwd(), os.path.dirname(JSON_outputfile))\n",
    "            ),\n",
    "        )\n",
    "        print(\n",
    "            f\"Loading Pickle for {world_countries[country_code]}\"\n",
    "        )  # TODO: Change to Logger\n",
    "    else:\n",
    "        create_elements = True\n",
    "        new_prefilter_data = True\n",
    "        print(\n",
    "            f\"Creating  New Elements for {world_countries[country_code]}\"\n",
    "        )  # TODO: Change to Logger\n",
    "\n",
    "    prefilter = {\n",
    "        Node: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "        Way: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "        Relation: {\"power\": [\"substation\", \"line\", \"generator\"]},\n",
    "    }  # see https://dlr-ve-esy.gitlab.io/esy-osmfilter/filter.html for filter structures\n",
    "    # HACKY: due to esy.osmfilter validation\n",
    "\n",
    "    blackfilter = [\n",
    "        (\"\", \"\"),\n",
    "    ]\n",
    "\n",
    "    for feature in [\"substation\", \"line\", \"generator\"]:\n",
    "        whitefilter = [\n",
    "            [\n",
    "                (\"power\", feature),\n",
    "            ],\n",
    "        ]\n",
    "        elementname = f\"{country_code}_{feature}s\"\n",
    "\n",
    "        feature_data = run_filter(\n",
    "            elementname,\n",
    "            PBF_inputfile,\n",
    "            JSON_outputfile,\n",
    "            prefilter,\n",
    "            whitefilter,\n",
    "            blackfilter,\n",
    "            NewPreFilterData=new_prefilter_data,\n",
    "            CreateElements=create_elements,\n",
    "            LoadElements=True,\n",
    "            verbose=False,\n",
    "            multiprocess=True,\n",
    "        )\n",
    "\n",
    "        if feature == \"substation\":\n",
    "            substation_data = feature_data\n",
    "        if feature == \"line\":\n",
    "            line_data = feature_data\n",
    "        if feature == \"generator\":\n",
    "            generator_data = feature_data\n",
    "\n",
    "    return (substation_data, line_data, generator_data)\n",
    "\n",
    "\n",
    "# Convert Ways to Point Coordinates\n",
    "\n",
    "\n",
    "# TODO: Use shapely and merge with convert_ways_lines\n",
    "def convert_ways_nodes(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    col = \"refs\"\n",
    "    df_way[col] = (\n",
    "        pd.Series().astype(float) if col not in df_way.columns else df_way[col]\n",
    "    )  # create empty \"refs\" if not in dataframe\n",
    "    for ref in df_way[\"refs\"]:\n",
    "        lonlats = []\n",
    "        for r in ref:\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\n",
    "            lonlats.append(lonlat)\n",
    "        lonlats = np.array(lonlats)\n",
    "        lonlat = np.mean(lonlats, axis=0)  # Hacky Apporx Centroid\n",
    "        lonlat_column.append(lonlat)\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)\n",
    "\n",
    "\n",
    "# Convert Ways to Line Coordinates\n",
    "\n",
    "\n",
    "def convert_ways_lines(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    for ref in df_way[\"refs\"]:  # goes through each row in df_way[\"refs\"]\n",
    "        lonlats = []\n",
    "        # picks each element in ref & replaces ID by coordinate tuple (A multiline consist of several points)\n",
    "        for r in ref:\n",
    "            # \"r\" is the ID in Data[\"Node\"], [\"lonlat\"] a list of [x1,y1] (coordinates)\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\n",
    "            lonlat = tuple(lonlat)\n",
    "            lonlats.append(lonlat)  # a list with tuples\n",
    "        lonlat_column.append(lonlats)  # adding a new list of tuples every row\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True)\n",
    "    df_way.insert(1, \"lonlat\", lonlat_column)\n",
    "\n",
    "\n",
    "# Convert Points Pandas Dataframe to GeoPandas Dataframe\n",
    "\n",
    "\n",
    "def convert_pd_to_gdf(df_way):\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_way, geometry=[Point(x, y) for x, y in df_way.lonlat], crs=\"EPSG:4326\"\n",
    "    )\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Convert Lines Pandas Dataframe to GeoPandas Dataframe\n",
    "\n",
    "\n",
    "def convert_pd_to_gdf_lines(df_way, simplified=False):\n",
    "    df_way[\"geometry\"] = df_way[\"lonlat\"].apply(lambda x: LineString(x))\n",
    "    if simplified is True:\n",
    "        df_way[\"geometry\"] = df_way[\"geometry\"].apply(\n",
    "            lambda x: x.simplify(0.005, preserve_topology=False)\n",
    "        )\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Convert Filtered Data, Elements to Pandas Dataframes\n",
    "\n",
    "\n",
    "def convert_filtered_data_to_dfs(country_code, feature_data, feature):\n",
    "    [Data, Elements] = feature_data\n",
    "    elementname = f\"{country_code}_{feature}s\"\n",
    "    df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\n",
    "    df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\n",
    "    return (df_node, df_way, Data)\n",
    "\n",
    "\n",
    "def process_substation_data(country_code, substation_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, substation_data, \"substation\"\n",
    "    )\n",
    "    convert_ways_nodes(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = world_countries[country_code]\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def process_line_data(country_code, line_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, line_data, \"line\"\n",
    "    )\n",
    "    convert_ways_lines(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    # Add Country Column\n",
    "    df_way[\"Country\"] = world_countries[country_code]\n",
    "    return df_way\n",
    "\n",
    "\n",
    "def process_generator_data(country_code, generator_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(\n",
    "        country_code, generator_data, \"generator\"\n",
    "    )\n",
    "    convert_ways_nodes(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = world_countries[country_code]\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def process_data():\n",
    "    df_all_substations = pd.DataFrame()\n",
    "    df_all_lines = pd.DataFrame()\n",
    "    df_all_generators = pd.DataFrame()\n",
    "    test_CC = {\"NG\": \"nigeria\"}\n",
    "    for country_code in test_CC.keys():\n",
    "        substation_data, line_data, generator_data = download_and_filter(country_code)\n",
    "        for feature in [\"substation\", \"line\", \"generator\"]:\n",
    "            if feature == \"substation\":\n",
    "                df_substation = process_substation_data(country_code, substation_data)\n",
    "                df_all_substations = pd.concat([df_all_substations, df_substation])\n",
    "            if feature == \"line\":\n",
    "                df_line = process_line_data(country_code, line_data)\n",
    "                df_all_lines = pd.concat([df_all_lines, df_line])\n",
    "            if feature == \"generator\":\n",
    "                df_generator = process_generator_data(country_code, generator_data)\n",
    "                df_all_generators = pd.concat([df_all_generators, df_generator])\n",
    "\n",
    "    # ----------- SUBSTATIONS -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_substations = df_all_substations[\n",
    "        df_all_substations.columns\n",
    "        & [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.substation\",\n",
    "            \"tags.voltage\",\n",
    "            \"tags.frequency\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    df_all_substations.drop(\n",
    "        df_all_substations.loc[\n",
    "            df_all_substations[\"tags.substation\"] == \"industrial\"\n",
    "        ].index,\n",
    "        inplace=True,\n",
    "    )  # Drop industrial substations\n",
    "    df_all_substations.drop(\n",
    "        df_all_substations.loc[\n",
    "            df_all_substations[\"tags.substation\"] == \"distribution\"\n",
    "        ].index,\n",
    "        inplace=True,\n",
    "    )  # Drop distribution substations\n",
    "\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_substations.\"\n",
    "    )\n",
    "    df_all_substations.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_substations = convert_pd_to_gdf(df_all_substations)\n",
    "    gdf_substations.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson\n",
    "\n",
    "    # ----------- LINES -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_lines = df_all_lines[\n",
    "        df_all_lines.columns\n",
    "        & [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.cables\",\n",
    "            \"tags.voltage\",\n",
    "            \"tags.circuits\",\n",
    "            \"tags.frequency\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_lines.\")\n",
    "    df_all_lines.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_lines = convert_pd_to_gdf_lines(df_all_lines, simplified=True)\n",
    "    gdf_lines.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson\n",
    "\n",
    "    # ----------- Generator -----------\n",
    "\n",
    "    # Columns of interest\n",
    "    df_all_generators = df_all_generators[\n",
    "        df_all_generators.columns\n",
    "        & [\n",
    "            \"id\",\n",
    "            \"lonlat\",\n",
    "            \"tags.power\",\n",
    "            \"tags.generator:type\",\n",
    "            \"tags.generator:method\",\n",
    "            \"tags.generator:source\",\n",
    "            \"tags.generator:output:electricity\",\n",
    "            \"Type\",\n",
    "            \"Country\",\n",
    "        ]\n",
    "    ]\n",
    "    # Generate Files\n",
    "    outputfile_partial = os.path.join(\n",
    "        os.getcwd(), \"data\", \"africa_all\" + \"_generators.\"\n",
    "    )\n",
    "    df_all_generators.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "    gdf_generators = convert_pd_to_gdf(df_all_generators)\n",
    "    gdf_generators.to_file(\n",
    "        outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    "    )  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overwrite Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function\n",
    "\n",
    "\n",
    "def lonlat_lookup(df_way, Data):\n",
    "    lonlat_list = []\n",
    "\n",
    "    col = \"refs\"\n",
    "    if col not in df_way.columns:\n",
    "        print(\"refs column not found\")\n",
    "        df_way[col] = pd.Series().astype(\n",
    "            float\n",
    "        )  # create empty \"refs\" if not in dataframe\n",
    "\n",
    "    for ref in df_way[\"refs\"]:\n",
    "        lonlat_row = []\n",
    "        for r in ref:\n",
    "            lonlat = tuple(Data[\"Node\"][str(r)][\"lonlat\"])\n",
    "            lonlat_row.append(lonlat)\n",
    "        lonlat_list.append(lonlat_row)\n",
    "    return lonlat_list\n",
    "\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def convert_ways_point(df_way, Data):\n",
    "    lonlat_list = lonlat_lookup(df_way, Data)\n",
    "    lonlat_column = []\n",
    "    area_column = []\n",
    "    for lonlat in lonlat_list:\n",
    "        way_polygon = Polygon(lonlat)\n",
    "        polygon_area = int(\n",
    "            round(\n",
    "                gpd.GeoSeries(way_polygon)\n",
    "                .set_crs(\"EPSG:4326\")\n",
    "                .to_crs(\"EPSG:3857\")\n",
    "                .area,\n",
    "                -1,\n",
    "            )\n",
    "        )  # nearest tens\n",
    "        # print('{:g}'.format(float('{:.3g}'.format(float(polygon_area))))) # For significant numbers\n",
    "        area_column.append(polygon_area)\n",
    "        center_point = way_polygon.centroid\n",
    "        lonlat_column.append(list((center_point.x, center_point.y)))\n",
    "\n",
    "    # df_way.drop(\"refs\", axis=1, inplace=True, errors=\"ignore\")\n",
    "    df_way.insert(0, \"lonlat\", lonlat_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations = pd.DataFrame()\n",
    "df_all_lines = pd.DataFrame()\n",
    "df_all_generators = pd.DataFrame()\n",
    "test_CC = {\"NA\": \"namibia\"}\n",
    "for country_code in test_CC.keys():\n",
    "    substation_data, line_data, generator_data = download_and_filter(country_code)\n",
    "    for feature in [\"substation\", \"line\", \"generator\"]:\n",
    "        if feature == \"substation\":\n",
    "            df_substation = process_substation_data(country_code, substation_data)\n",
    "            df_all_substations = pd.concat([df_all_substations, df_substation])\n",
    "        if feature == \"line\":\n",
    "            df_line = process_line_data(country_code, line_data)\n",
    "            df_all_lines = pd.concat([df_all_lines, df_line])\n",
    "        if feature == \"generator\":\n",
    "            df_generator = process_generator_data(country_code, generator_data)\n",
    "            df_all_generators = pd.concat([df_all_generators, df_generator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- SUBSTATIONS -----------\n",
    "\n",
    "# Clean\n",
    "df_all_substations.reset_index(drop=True, inplace=True)\n",
    "df_all_substations.dropna(\n",
    "    thresh=len(df_all_substations) * 0.25, axis=1, how=\"all\", inplace=True\n",
    ")  # Drop Columns with 75% values as N/A\n",
    "df_all_substations.dropna(\n",
    "    subset=[\"tags.voltage\"], inplace=True\n",
    ")  # Drop any substations with Voltage = N/A\n",
    "df_all_substations.drop(\n",
    "    df_all_substations.loc[df_all_substations[\"tags.substation\"] == \"industrial\"].index,\n",
    "    inplace=True,\n",
    ")\n",
    "df_all_substations.drop(\n",
    "    df_all_substations.loc[\n",
    "        df_all_substations[\"tags.substation\"] == \"distribution\"\n",
    "    ].index,\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_substations.\")\n",
    "df_all_substations.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "gdf_substations = convert_pd_to_gdf(df_all_substations.drop(\"refs\", 1))\n",
    "gdf_substations.to_file(\n",
    "    outputfile_partial + \"geojson\", driver=\"GeoJSON\"\n",
    ")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_substations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Generator -----------\n",
    "\n",
    "df_all_generators.reset_index(drop=True, inplace=True)\n",
    "df_all_generators.drop(\n",
    "    columns=[\n",
    "        \"tags.fixme\",\n",
    "        \"tags.frequency\",\n",
    "        \"tags.name:ar\",\n",
    "        \"tags.building\",\n",
    "        \"tags.barrier\",\n",
    "    ],\n",
    "    inplace=True,\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "df_all_generators = df_all_generators[\n",
    "    df_all_generators[\"tags.generator:output:electricity\"]\n",
    "    .astype(str)\n",
    "    .str.contains(\"MW\")\n",
    "]  # removes boolean\n",
    "df_all_generators[\"tags.generator:output:electricity\"] = (\n",
    "    df_all_generators[\"tags.generator:output:electricity\"]\n",
    "    .str.extract(\"(\\d+)\")\n",
    "    .astype(float)\n",
    ")\n",
    "df_all_generators.rename(\n",
    "    columns={\"tags.generator:output:electricity\": \"power_output_MW\"}, inplace=True\n",
    ")\n",
    "# df_all_generators.dropna(thresh=len(df_all_generators)*0.25, axis=1, how='all', inplace=True) # Drop Columns with 75% values as N/A\n",
    "\n",
    "# # Generate Files\n",
    "# outputfile_partial = os.path.join(os.getcwd(),'data','africa_all'+'_generators.')\n",
    "# df_all_generators.to_csv(outputfile_partial + 'csv') # Generate CSV\n",
    "# gdf_generators = convert_pd_to_gdf(df_all_generators)\n",
    "# gdf_generators.to_file(outputfile_partial+'geojson', driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- LINES -----------\n",
    "\n",
    "# Clean\n",
    "# TODO: FIX Voltage Filter\n",
    "# Some transmission lines carry multiple voltages, having voltage_V = 10000;20000  (two lines)\n",
    "# The following code keeps only the first information before the semicolon..\n",
    "# Needs to be corrected in future, creating two lines with the same bus ID.\n",
    "\n",
    "\n",
    "df_all_lines = df_all_lines[\n",
    "    {\n",
    "        \"id\",\n",
    "        \"refs\",\n",
    "        \"lonlat\",\n",
    "        \"tags.power\",\n",
    "        \"tags.cables\",\n",
    "        \"tags.voltage\",\n",
    "        \"tags.frequency\",\n",
    "        \"Type\",\n",
    "        \"Country\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Clean data\n",
    "df_all_lines = df_all_lines.reset_index(drop=True)\n",
    "df_all_lines = df_all_lines.dropna(\n",
    "    subset=[\"tags.voltage\"]\n",
    ")  # Drop any lines with Voltage = N/A\n",
    "df_all_lines = df_all_lines.rename(columns={\"tags.voltage\": \"voltage_V\"})\n",
    "df_all_lines[\"voltage_V\"] = (\n",
    "    df_all_lines[\"voltage_V\"].str.split(\"*\").str[0]\n",
    ")  # just keeps the\n",
    "df_all_lines[\"voltage_V\"] = df_all_lines[\"voltage_V\"].str.split(\";\").str[0]\n",
    "df_all_lines[\"voltage_V\"] = (\n",
    "    df_all_lines[\"voltage_V\"]\n",
    "    .apply(lambda x: pd.to_numeric(x, errors=\"coerce\"))\n",
    "    .dropna()\n",
    ")  ## if cell can't converted to float -> drop\n",
    "df_all_lines = df_all_lines[df_all_lines.voltage_V > 10000]\n",
    "# df_all_lines['end_refs'] =\n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_lines.\")\n",
    "df_all_lines.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "gdf_lines = convert_pd_to_gdf_lines(df_all_lines.drop(\"refs\", 1), simplified=True)\n",
    "gdf_lines.to_file(outputfile_partial + \"geojson\", driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_line_lookup = df_all_lines[[\"id\", \"refs\"]]\n",
    "display(df_line_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_node_lookup = df_all_substations[[\"id\", \"refs\"]]\n",
    "df_node_lookup = df_node_lookup.dropna(\n",
    "    subset=[\"refs\"]\n",
    ")  # Drop any nodes with refs = N/A\n",
    "display(df_node_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "    out = []\n",
    "    for n, row in df.iterrows():\n",
    "        for item in row[\"refs\"]:\n",
    "            row[\"flat_ref\"] = item\n",
    "            out += [row.copy()]\n",
    "\n",
    "    flattened_df = pd.DataFrame(out)\n",
    "    flattened_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return flattened_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "way_f = flatten_df(df_line_lookup)\n",
    "display(way_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_f = flatten_df(df_node_lookup)\n",
    "display(node_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = way_f.merge(node_f, on=\"flat_ref\", how=\"inner\")\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_line_refs = set()\n",
    "for ref in df_all_lines[\"refs\"]:  # goes through each row in df_way['refs']\n",
    "    for r in ref:\n",
    "        all_line_refs.add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_st_refs = set()\n",
    "for ref in df_all_substations.dropna(subset=[\"refs\"])[\n",
    "    \"refs\"\n",
    "]:  # goes through each row in df_way['refs']\n",
    "    for r in ref:\n",
    "        all_st_refs.add(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_line_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.geometry.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot raw data\n",
    "\n",
    "\n",
    "continent = gpd.read_file(\"resources/country_shapes.geojson\")\n",
    "off_shore = gpd.read_file(\"resources/offshore_shapes.geojson\")\n",
    "off_shore_old = gpd.read_file(\"resources/offshore_shapes_old.geojson\")\n",
    "gadm = gpd.read_file(\"resources/gadm_shapes.geojson\")\n",
    "\n",
    "df_all_lines2 = df_all_lines.copy()\n",
    "df_all_lines2 = df_all_lines2[~df_all_lines2[\"tags.voltage\"].isna()]\n",
    "df_all_lines2[\"tags.voltage\"] = (\n",
    "    df_all_lines2[\"tags.voltage\"]\n",
    "    .apply(lambda x: pd.to_numeric(x, errors=\"coerce\"))\n",
    "    .astype(float)\n",
    ")\n",
    "df_all_lines2 = df_all_lines2[df_all_lines2[\"tags.voltage\"] >= 110000]\n",
    "df_all_lines2[\"geometry\"] = [\n",
    "    LineString(x.lonlat) for index, x in df_all_lines2.iterrows()\n",
    "]\n",
    "lines = gpd.GeoDataFrame(geometry=df_all_lines2.geometry, crs=\"epsg:4326\")\n",
    "\n",
    "ax = continent.plot(figsize=(10, 10))\n",
    "off_shore.plot(ax=ax)\n",
    "lines.plot(ax=ax, color=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_lines2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64a9945f8f137ed15f7097a1dbedf9f1ce29494f0e33cb4fc5724026c999b930"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
